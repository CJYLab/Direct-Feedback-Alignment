{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIRECT FEEDBACK ALIGNMENT ON MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils.np_utils import to_categorical\n",
    "np.random.seed(1234)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10bbe6590>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAFdCAYAAAA9hbc/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWJKSGMCSlOT/AFeM8W+oZeYUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10da03350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n",
      "(60000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape, X_test.shape\n",
    "print y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(60000, 28*28)\n",
    "X_test = X_test.reshape(10000, 28*28)\n",
    "\n",
    "print X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "y_train = to_categorical(y_train, nb_classes)\n",
    "y_test = to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(W1, W2, b1, b2, x):\n",
    "    '''This is the forward pass. It is equal for any\n",
    "    training algorithm. It's just one hidden layer\n",
    "    with tanh activation function and sigmoid on the\n",
    "    output layer'''\n",
    "    a1 = np.matmul(W1, x)+b1 # usare np.tile\n",
    "    h1 = np.tanh(a1)\n",
    "    a2 = np.matmul(W2, h1)+b2 #usare np.tile\n",
    "    y_hat = expit(a2)\n",
    "    return a1, h1, a2, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 1)\n"
     ]
    }
   ],
   "source": [
    "input = X_train[0]\n",
    "input = input[:, np.newaxis]\n",
    "print input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1, W2 = np.random.randn(800, 784), np.random.randn(10, 800)\n",
    "b1, b2 = np.random.randn(800, 1), np.random.randn(10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a1, h1, a2, y_hat = forward_pass(W1, W2, b1, b2, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.7855264408909708"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_train[0], y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backprop_backward_pass(e, h1, W2, a1, x):\n",
    "    dW2 = -np.matmul(e, np.transpose(h1))\n",
    "    da1 = np.matmul(np.transpose(W2), e)*(1-np.tanh(a1)**2)\n",
    "    dW1 = -np.matmul(da1, np.transpose(x))\n",
    "    db1 = -da1\n",
    "    db2 = -e\n",
    "    return dW1, dW2, db1, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e = y_hat - y_train[0][:, np.newaxis]\n",
    "dW1, dW2, db1, db2 = backprop_backward_pass(e, h1, W2, a1, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(x, y, n_epochs=10, lr=1e-3, batch_size=200, tol=1e-1):\n",
    "    x = np.transpose(x)\n",
    "    y = np.transpose(y)\n",
    "    \n",
    "    W1, W2 = np.random.randn(800, 784), np.random.randn(10, 800)\n",
    "    b1, b2 = np.random.randn(800, 1), np.random.randn(10, 1)\n",
    "    \n",
    "    dataset_size = x.shape[1]\n",
    "    n_batches = dataset_size//batch_size\n",
    "    \n",
    "    for i in xrange(n_epochs):\n",
    "        perm = np.random.permutation(x.shape[1])\n",
    "        x = x[:, perm]\n",
    "        y = y[:, perm]\n",
    "        #plt.imshow(x[:, 0].reshape(28, 28))\n",
    "        #print np.argmax(y, axis=0)[0]\n",
    "        loss = 0.\n",
    "        train_error = 0.\n",
    "        for j in xrange(n_batches):\n",
    "            if j%50==0:\n",
    "                print 'Batch #', j\n",
    "            samples = x[:, j*batch_size:(j+1)*batch_size]\n",
    "            targets = y[:, j*batch_size:(j+1)*batch_size]\n",
    "            a1, h1, a2, y_hat = forward_pass(W1, W2, b1, b2, samples) #modificare forward_pass e usare np.tile per\n",
    "            # aggiungere b1 e b2 ---- +np.tile(b1, batch_size)\n",
    "            error = y_hat - targets\n",
    "            preds = np.argmax(y_hat, axis=0) \n",
    "            truth = np.argmax(targets, axis=0)\n",
    "            train_error += 1.*np.sum(preds!=truth)\n",
    "            loss_on_batch = log_loss(targets, y_hat)\n",
    "            \n",
    "            dW1, dW2, db1, db2 = backprop_backward_pass(error, h1, W2, a1, samples)\n",
    "            W1 += lr*dW1\n",
    "            W2 += lr*dW2\n",
    "            b2 += lr*error\n",
    "            b1 += lr*db1\n",
    "            loss += loss_on_batch\n",
    "        print 'Loss at epoch', i, ':', loss/x.shape[1]\n",
    "        print 'Training error:', train_error/x.shape[1]\n",
    "        if loss <= tol:\n",
    "            break\n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 0 : 0.683592805046\n",
      "Training error: 0.219966666667\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 1 : 0.4814483549\n",
      "Training error: 0.113416666667\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 2 : 0.438704416089\n",
      "Training error: 0.0918\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 3 : 0.413186203122\n",
      "Training error: 0.0776\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 4 : 0.396604832907\n",
      "Training error: 0.0686833333333\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 5 : 0.384245381995\n",
      "Training error: 0.0619833333333\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 6 : 0.373785547355\n",
      "Training error: 0.0563333333333\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 7 : 0.365142973738\n",
      "Training error: 0.0506833333333\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 8 : 0.359701104089\n",
      "Training error: 0.0464666666667\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 9 : 0.354424089779\n",
      "Training error: 0.0441333333333\n"
     ]
    }
   ],
   "source": [
    "nW1, nW2, nb1, nb2 = train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dfa_backward_pass(e, h1, B1, a1, x):\n",
    "    dW2 = -np.matmul(e, np.transpose(h1))\n",
    "    da1 = np.matmul(B1, e)*(1-np.tanh(a1)**2)\n",
    "    dW1 = -np.matmul(da1, np.transpose(x))\n",
    "    db1 = -da1\n",
    "    db2 = -e\n",
    "    return dW1, dW2, db1, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dfa_train(x, y, n_epochs=10, lr=1e-3, batch_size=200, tol=1e-1):\n",
    "    x = np.transpose(x)\n",
    "    y = np.transpose(y)\n",
    "    \n",
    "    W1, W2 = np.random.randn(800, 784), np.random.randn(10, 800)\n",
    "    b1, b2 = np.random.randn(800, batch_size), np.random.randn(10, batch_size)\n",
    "    \n",
    "    B1 = np.random.randn(800, 10)\n",
    "    dataset_size = x.shape[1]\n",
    "    n_batches = dataset_size//batch_size\n",
    "    \n",
    "    for i in xrange(n_epochs):\n",
    "        perm = np.random.permutation(x.shape[1])\n",
    "        x = x[:, perm]\n",
    "        y = y[:, perm]\n",
    "        #plt.imshow(x[:, 0].reshape(28, 28))\n",
    "        #print np.argmax(y, axis=0)[0]\n",
    "        loss = 0.\n",
    "        train_error = 0.\n",
    "        for j in xrange(n_batches):\n",
    "            if j%50==0:\n",
    "                print 'Batch #', j\n",
    "            samples = x[:, j*batch_size:(j+1)*batch_size]\n",
    "            targets = y[:, j*batch_size:(j+1)*batch_size]\n",
    "            a1, h1, a2, y_hat = forward_pass(W1, W2, b1, b2, samples)\n",
    "            error = y_hat - targets\n",
    "            preds = np.argmax(y_hat, axis=0) \n",
    "            truth = np.argmax(targets, axis=0)\n",
    "            train_error += 1.*np.sum(preds!=truth)\n",
    "            loss_on_batch = log_loss(targets, y_hat)\n",
    "            \n",
    "            dW1, dW2, db1, db2 = dfa_backward_pass(error, h1, B1, a1, samples)\n",
    "            W1 += lr*dW1\n",
    "            W2 += lr*dW2\n",
    "            b2 += lr*error\n",
    "            b1 += lr*db1\n",
    "            loss += loss_on_batch\n",
    "        print 'Loss at epoch', i, ':', loss/x.shape[1]\n",
    "        print 'Training error:', train_error/x.shape[1]\n",
    "        if loss <= tol:\n",
    "            break\n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 0 : 0.683397809826\n",
      "Training error: 0.2211\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 1 : 0.476418312413\n",
      "Training error: 0.111483333333\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 2 : 0.432522037757\n",
      "Training error: 0.08885\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 3 : 0.408683172545\n",
      "Training error: 0.0751333333333\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 4 : 0.39164081044\n",
      "Training error: 0.06635\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 5 : 0.379940137792\n",
      "Training error: 0.0598666666667\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 6 : 0.370439574007\n",
      "Training error: 0.05435\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 7 : 0.362527801871\n",
      "Training error: 0.0483166666667\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 8 : 0.356886926982\n",
      "Training error: 0.0452333333333\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 9 : 0.350549964779\n",
      "Training error: 0.0425\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 10 : 0.347061652416\n",
      "Training error: 0.0396333333333\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 11 : 0.343209814346\n",
      "Training error: 0.03755\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 12 : 0.340525021941\n",
      "Training error: 0.0346\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 13 : 0.337159900682\n",
      "Training error: 0.0329666666667\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 14 : 0.33465994062\n",
      "Training error: 0.0303333333333\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 15 : 0.332118775723\n",
      "Training error: 0.0291166666667\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 16 : 0.330266065483\n",
      "Training error: 0.0274166666667\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 17 : 0.328826798728\n",
      "Training error: 0.0262333333333\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 18 : 0.326702968778\n",
      "Training error: 0.0245666666667\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 19 : 0.325919144958\n",
      "Training error: 0.0233333333333\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 20 : 0.324236916373\n",
      "Training error: 0.02185\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 21 : 0.323003051473\n",
      "Training error: 0.02105\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 22 : 0.32217113687\n",
      "Training error: 0.02105\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 23 : 0.320812564654\n",
      "Training error: 0.0194166666667\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 24 : 0.319338189115\n",
      "Training error: 0.01855\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 25 : 0.318834044468\n",
      "Training error: 0.01765\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 26 : 0.318007352937\n",
      "Training error: 0.0169833333333\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 27 : 0.317081252587\n",
      "Training error: 0.0158\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 28 : 0.316659572111\n",
      "Training error: 0.01575\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 29 : 0.315553363042\n",
      "Training error: 0.0146166666667\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 30 : 0.315639346691\n",
      "Training error: 0.01435\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 31 : 0.31453484466\n",
      "Training error: 0.0137166666667\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 32 : 0.314243972838\n",
      "Training error: 0.01315\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 33 : 0.31356936528\n",
      "Training error: 0.0123333333333\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 34 : 0.313235018091\n",
      "Training error: 0.0123333333333\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 35 : 0.312601422189\n",
      "Training error: 0.01145\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 36 : 0.312403757236\n",
      "Training error: 0.0117666666667\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 37 : 0.312249909909\n",
      "Training error: 0.01135\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 38 : 0.311569830719\n",
      "Training error: 0.01005\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 39 : 0.311006308887\n",
      "Training error: 0.0102333333333\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 40 : 0.310934512613\n",
      "Training error: 0.0096\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 41 : 0.31033620073\n",
      "Training error: 0.00946666666667\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 42 : 0.310242764073\n",
      "Training error: 0.00911666666667\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 43 : 0.309813696431\n",
      "Training error: 0.00871666666667\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 44 : 0.309736351379\n",
      "Training error: 0.0083\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 45 : 0.309190441409\n",
      "Training error: 0.0075\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 46 : 0.309144170541\n",
      "Training error: 0.00728333333333\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 47 : 0.308914616744\n",
      "Training error: 0.00806666666667\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 48 : 0.308906028151\n",
      "Training error: 0.00736666666667\n",
      "Batch # 0\n",
      "Batch # 50\n",
      "Batch # 100\n",
      "Batch # 150\n",
      "Batch # 200\n",
      "Batch # 250\n",
      "Loss at epoch 49 : 0.308650600583\n",
      "Training error: 0.00708333333333\n"
     ]
    }
   ],
   "source": [
    "nW1, nW2, nb1, nb2 = train(X_train, y_train, n_epochs=50, lr=1e-3, batch_size=200, tol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(W1, W2, test_samples, test_targets):\n",
    "    test_samples = np.transpose(test_samples)\n",
    "    test_targets = np.transpose(test_targets)\n",
    "    b1 = np.zeros((800, 10000))\n",
    "    b2 = np.zeros((10, 10000))\n",
    "    outs = forward_pass(W1, W2, b1, b2, test_samples)[-1]\n",
    "    preds = np.argmax(outs, axis=0) \n",
    "    truth = np.argmax(test_targets, axis=0)\n",
    "    test_error = 1.*np.sum(preds!=truth)/preds.shape[0]\n",
    "    return test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0447"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(nW1, nW2, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
